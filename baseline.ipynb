{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYhA4mBR74E+rIw4RH+lYR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gg-dema/geometric-algebraic-transformer/blob/main/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline\n"
      ],
      "metadata": {
        "id": "C8kzbHsA9Lwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataModule"
      ],
      "metadata": {
        "id": "-d4yIg2zKJj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PureFeaturesDataset(Dataset):\n",
        "    def __init__(self, single_tensor, bifurcation_tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mv_tensor (torch.Tensor): Tensor with shape (n_items, 3, 16)\n",
        "        \"\"\"\n",
        "        super(PureFeaturesDataset, self).__init__()\n",
        "        labels_single = torch.zeros(single_tensor.size(0))\n",
        "        labels_bifurcating = torch.ones(bifurcation_tensor.size(0))\n",
        "\n",
        "        self.data = torch.cat((single_tensor, bifurcating_tensor), dim=0)\n",
        "        self.data = F.normalize(self.data, p=2.0, dim=-1)\n",
        "        self.labels = torch.cat((labels_single, labels_bifurcating), dim=0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "#\n",
        "class PureFeaturesDataModule(pl.LightningDataModule):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      mv_data (MultivectorDataset): dataset containing all multivectors;\n",
        "      batch_size (int): size of the batches during training and testing\n",
        "\n",
        "    Attributes:\n",
        "      mv_data (MultivectorDataset): dataset containing all multivectors;\n",
        "      batch_size (int): size of the batches during training and testing;\n",
        "      data_train (multivector dataset): dataset for train;\n",
        "      data_val (multivector dataset): dataset for validation;\n",
        "      data_test (multivector dataset): dataset for test;\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 single_tensor,\n",
        "                 bifurcating_tensor,\n",
        "                 batch_size=32,\n",
        "                 ):\n",
        "\n",
        "      super(PureFeaturesDataModule,self).__init__()\n",
        "      self.single_tensor = single_tensor\n",
        "      self.bifurcating_tensor =  bifurcating_tensor\n",
        "      self.batch_size = batch_size\n",
        "\n",
        "    def setup(self, stage = None):\n",
        "\n",
        "      if stage == 'fit' or stage is None:\n",
        "            dataset = PureFeaturesDataset(self.single_tensor, self.bifurcating_tensor)\n",
        "            train_len = dataset.__len__()\n",
        "            val_len = int(0.1*train_len)\n",
        "            val_len_split = [train_len - val_len,val_len]\n",
        "            self.data_train, self.data_val = random_split(dataset, val_len_split)\n",
        "      elif stage == 'test':\n",
        "            self.data_test = MultivectorDataset(filename = self.test_data_name, gatr_flag = self.gatr_flag)\n",
        "      elif stage == 'predict':\n",
        "            self.data_test = MultivectorDataset(filename = self.test_data_name, gatr_flag = self.gatr_flag)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.data_train, shuffle=True, batch_size=self.batch_size, num_workers=0)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.data_val, shuffle=False, batch_size=self.batch_size, num_workers=0)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.data_test, shuffle=False, batch_size=self.batch_size, num_workers=0)\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        return DataLoader(self.data_test, shuffle=False, batch_size=self.batch_size, num_workers=0)"
      ],
      "metadata": {
        "id": "gKMEgUplKIiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layers"
      ],
      "metadata": {
        "id": "3F1cu5SG9SDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##Embedding\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, seq_len=50):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "\n",
        "        embedd = torch.zeros(seq_len, input_dim)\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, input_dim, 2).float() * (-math.log(10000.0) / input_dim))\n",
        "        embedd[:, 0::2] = torch.sin(position * div_term)\n",
        "        embedd[:, 1::2] = torch.cos(position * div_term)\n",
        "        embedd = embedd.unsqueeze(0)\n",
        "        self.register_buffer('embedding', embedd)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.embedding\n",
        "        return out\n",
        "\n",
        "PositionalEmbedding(16)"
      ],
      "metadata": {
        "id": "7ITnUuTu930d",
        "outputId": "3fbea891-fc19-42ed-e29d-29cf7c00bdd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PositionalEmbedding()"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHead(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim, num_heads):\n",
        "\n",
        "        super(MultiHead, self).__init__()\n",
        "        assert emb_dim % num_heads == 0, \"latent dimension must be divisible by the number of heads\"\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = int(self.emb_dim/self.num_heads)\n",
        "        # projection matrix in the QKV space: each matrix is emb/num_head x emb/num_head\n",
        "        # Expected dimension:  64/4 : 16x16 matrix\n",
        "        self.q_matrix = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.k_matrix = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.v_matrix = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "\n",
        "        # output layer, take the concatenation of the multiheads output and\n",
        "        # reproject with the same dimenison of the input\n",
        "        self.output_projection = nn.Linear(self.num_heads*self.head_dim, self.emb_dim)\n",
        "\n",
        "        #self._reset_parameters()\n",
        "\n",
        "\n",
        "    # why we do that? why is usefull to have this data distribution?\n",
        "    def _reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.qkv_projection.weight)\n",
        "        self.qkv_projection.bias.data.fill_(0)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.my_projection.weight)\n",
        "        self.my_projection.bias.data.fill_(0)\n",
        "\n",
        "    # ps: the mask is for the decoder or for hide some token\n",
        "    def forward(self, k, q, v, mask=None):\n",
        "        batch_size, seq_len = k.size(0), k.size(1)\n",
        "        # transpose all\n",
        "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Project : output :   [batch, seq_len, num_heads, head_dim]\n",
        "        K = self.k_matrix(k) #  batch, 50, 4, 16\n",
        "        Q = self.q_matrix(q)\n",
        "        V = self.v_matrix(v)\n",
        "\n",
        "        # for QxK.t we should re-arrange the view:\n",
        "        # (batch_size, n_heads, seq_len, head_dim)\n",
        "        Q = Q.transpose(1,2)  #  (32 x 4 x 50 x 16)\n",
        "        K = K.transpose(1,2)  #  (32 x 4 x 50 x 16)\n",
        "        V = V.transpose(1,2)  #  (32 x 4 x 50 x 16)\n",
        "\n",
        "        K_transpose = K.transpose(-1, -2) # (batch_size, n_heads, head_dim, seq_len)\n",
        "        #print(f\"k_transp{K_transpose.shape}\")\n",
        "\n",
        "        score = torch.matmul(Q, K_transpose)\n",
        "        # score = (batch_size, n_heads, seq_len, seq_len)\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask==0, float('-1e20'))\n",
        "\n",
        "        attention_score = F.softmax(score/math.sqrt(self.head_dim), dim=-1)\n",
        "        attention = torch.matmul(attention_score, V)\n",
        "        # remove a contiuguous call after the transpose, it should just worry in case of memory proble\n",
        "        concat_multihead = attention.transpose(1, 2)\n",
        "        concat_multihead = concat_multihead.reshape(\n",
        "            batch_size,\n",
        "            seq_len,\n",
        "            self.num_heads * self.head_dim)\n",
        "        output = self.output_projection(concat_multihead)\n",
        "        return output\n",
        "\n",
        "MultiHead(emb_dim=64, num_heads=4)\n"
      ],
      "metadata": {
        "id": "IHYcARbY97YK",
        "outputId": "a7f9de5d-602a-4676-ea54-1de685f68c6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiHead(\n",
              "  (q_matrix): Linear(in_features=16, out_features=16, bias=False)\n",
              "  (k_matrix): Linear(in_features=16, out_features=16, bias=False)\n",
              "  (v_matrix): Linear(in_features=16, out_features=16, bias=False)\n",
              "  (output_projection): Linear(in_features=64, out_features=64, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "Fc7F_ifQ-mi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class WarmupScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
        "\n",
        "    def __init__(self, optimizer, warmup, max_number_iters=-1):\n",
        "        self.warmup = warmup\n",
        "        self.max_number_iters = max_number_iters\n",
        "        super(WarmupScheduler, self).__init__(optimizer)\n",
        "\n",
        "\n",
        "    def get_lr(self):\n",
        "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
        "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
        "\n",
        "    def get_lr_factor(self, epoch):\n",
        "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch  / self.max_number_iters))\n",
        "        if epoch < self.warmup:\n",
        "            lr_factor *= epoch / self.warmup\n",
        "        return lr_factor\n"
      ],
      "metadata": {
        "id": "LJEbvz-v-lVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "9fUWPXWs9euP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, projected_dim, num_heads=4, dropout=0.5):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.project = nn.Linear(input_dim, projected_dim)\n",
        "    self.embedding = PositionalEmbedding(projected_dim)\n",
        "    self.multi_head_block = MultiHead(projected_dim, num_heads)\n",
        "    self.layer_norm1 = nn.LayerNorm(projected_dim)\n",
        "\n",
        "    self.fully_connected = nn.Sequential(\n",
        "        nn.Linear(projected_dim, projected_dim//2),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.LayerNorm(projected_dim//2),\n",
        "        nn.Linear(projected_dim//2, 64)\n",
        "    )\n",
        "    self.logit_layer = nn.Linear(64, 1)\n",
        "    self.activation = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    x : [batch, seq_len, input_dim]\n",
        "    latent_rappresentation = [batch, seq_len, num_heads, head_dimension]\n",
        "    \"\"\"\n",
        "    if x.ndim == 4:\n",
        "      x = x.squeeze(2)\n",
        "    emb = self.embedding(self.project(x))                    # embedding\n",
        "    output_encoder = self.multi_head_block(emb, emb, emb)    # multihead\n",
        "    y = self.layer_norm1(output_encoder + emb)               # residual + layerNorm\n",
        "    x = self.activation(self.fully_connected(y))             # classifier\n",
        "    x = torch.mean(x, 1)                                     # mean pooling\n",
        "    x = self.logit_layer(x)                                  # logit\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "i5ZFEfSs-s4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TransformerArchitecture(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, input_dim=3, projected_dim=128):\n",
        "        super(TransformerArchitecture, self).__init__()\n",
        "\n",
        "        # training parameters:\n",
        "        self.lr = 1e-3\n",
        "        self.warmup = 0.01\n",
        "        self.max_iters = 1000\n",
        "        self.net = Transformer(\n",
        "            input_dim=input_dim,\n",
        "            projected_dim=projected_dim,\n",
        "            num_heads=4,\n",
        "            dropout=0.5)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.accuracy_meter = torchmetrics.classification.Accuracy(\n",
        "            task = \"binary\",\n",
        "            num_classes = 2\n",
        "         )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n",
        "        \"\"\"\n",
        "        Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
        "        Input arguments same as the forward pass.\n",
        "        \"\"\"\n",
        "        x = self.input_net(x)\n",
        "        if add_positional_encoding:\n",
        "            x = self.positional_encoding(x)\n",
        "        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n",
        "        return attention_maps\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "        # Apply lr scheduler per step\n",
        "        lr_scheduler = WarmupScheduler(optimizer,\n",
        "                                        warmup=self.warmup,\n",
        "                                        max_number_iters=self.max_iters)\n",
        "\n",
        "        return [optimizer], [{'scheduler': lr_scheduler, 'interval': 'step'}]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "\n",
        "        inputs, labels = batch\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = self.net(inputs).squeeze(-1)\n",
        "\n",
        "        probabilities = self.sigmoid(outputs)\n",
        "        train_loss = self.criterion(probabilities,labels.float())\n",
        "        train_accuracy = self.accuracy_meter(probabilities,labels.float())\n",
        "\n",
        "        values = {\"train_loss\": train_loss, \"train_acc\": train_accuracy}\n",
        "        self.log_dict(values, prog_bar = True)\n",
        "\n",
        "        return train_loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "\n",
        "        inputs, labels = batch\n",
        "        inputs, labels = inputs.to(device),labels.to(device)\n",
        "        outputs = self.net(inputs).squeeze(-1)\n",
        "\n",
        "        probabilities = self.sigmoid(outputs)\n",
        "        val_loss = self.criterion(probabilities,labels.float())\n",
        "        val_accuracy = self.accuracy_meter(probabilities,labels.float())\n",
        "\n",
        "        values = {\"val_loss\": val_loss, \"val_acc\": val_accuracy}\n",
        "        self.log_dict(values, prog_bar = True)\n",
        "\n",
        "        return val_loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = self.net(inputs).squeeze(-1)\n",
        "\n",
        "        probabilities = self.sigmoid(outputs)\n",
        "        test_loss = self.criterion(probabilities,labels.float())\n",
        "        test_accuracy = self.accuracy_meter(probabilities,labels.float())\n",
        "\n",
        "        values = {\"test_loss\": test_loss, \"test_acc\": test_accuracy}\n",
        "        self.log_dict(values, prog_bar = True)\n",
        "\n",
        "        return test_loss\n"
      ],
      "metadata": {
        "id": "trMidoIc-0Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Evaluation"
      ],
      "metadata": {
        "id": "XMbHSQfn9q6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pure_features_datamodule = PureFeaturesDataModule(\n",
        "    batch_size = 125,\n",
        "    single_tensor = single_tensor,\n",
        "    bifurcating_tensor = bifurcating_tensor\n",
        ")\n",
        "transformer_model = TransformerArchitecture(input_dim=3,\n",
        "                                            projected_dim=8).to(device)\n",
        "pure_features_datamodule.setup(None)\n",
        "dataloader_train = pure_features_datamodule.train_dataloader()\n",
        "pl.Trainer(max_epochs=1).fit(transformer_model, pure_features_datamodule)\n"
      ],
      "metadata": {
        "id": "-Z8Jv54s-2MW",
        "outputId": "3c49a86a-bfb1-4bf1-cb3b-66d24c06f09a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416,
          "referenced_widgets": [
            "b55e255a9b9a498294a4f7f586df3054",
            "8dc3b1e733884053a40a4dbd1c85ae66",
            "5b277b147ed34810a0ce358412e811f5",
            "d01402bc32fb4f538dec41fd6ca1e934",
            "992d4252bb4042599e00b5751173c455",
            "4038d82f0ffc4fafb12bc5be67512ca7",
            "3aa872ae3e6c4ea3bea320cd51e15f4c",
            "7f61b893dbf841fa830bdb5b4de11dc9",
            "830849da57ca4ca7ac492b4053d38ea0",
            "bb6622cc741945899be5838d624be69e",
            "1f63645e34e84158bb545d91ce37bb4c",
            "a24fa808e4c9480ca2b294679a8087a6",
            "9613e9bf24eb44d1bdb051b91dce85fe",
            "98b8f63b832e405db046c81ae2234053",
            "a085403943ba4c6483b7407b8ca810c0",
            "bbbd0303e1304b6f9d32a327e6311782",
            "7d659917a97a46fbacdf6cf6b7184a8c",
            "6821abfe931f4731a4b1bcac4c52a076",
            "7b422831de0347a6b0b13aef3a79abbf",
            "2ef893dea4b341449aa9081d3d341192",
            "f3249335d7be4b9e9425b01b605b42d3",
            "cac53755957b4394ae22aa8a93635a9d",
            "a19e065a64884ee39f17b6a5e5c3a905",
            "afef36dd32e746dfa31eb7578b1c1434",
            "c3d6414ed7bd440385b666860b73652c",
            "d290cb1e9ddb41b69993782b775bce7b",
            "111d762e003e4cb69190c7092947068c",
            "0bc38a38f62e477780135510f192bc77",
            "15c60f72b971426ea9fc74a04894fef3",
            "74008c590a684434a3f0b4b9d9eb0512",
            "f1e9621768dc40368dd898ae69077087",
            "b1d951db30f34ca2b5e67cfa3034c8f6",
            "70ab2f478f554bbcbec1f563467aabcd"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name           | Type           | Params | Mode \n",
            "----------------------------------------------------------\n",
            "0 | net            | Transformer    | 561    | train\n",
            "1 | sigmoid        | Sigmoid        | 0      | train\n",
            "2 | criterion      | BCELoss        | 0      | train\n",
            "3 | accuracy_meter | BinaryAccuracy | 0      | train\n",
            "----------------------------------------------------------\n",
            "561       Trainable params\n",
            "0         Non-trainable params\n",
            "561       Total params\n",
            "0.002     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b55e255a9b9a498294a4f7f586df3054"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:298: PossibleUserWarning:\n",
            "\n",
            "The number of training batches (29) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a24fa808e4c9480ca2b294679a8087a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a19e065a64884ee39f17b6a5e5c3a905"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1` reached.\n"
          ]
        }
      ]
    }
  ]
}